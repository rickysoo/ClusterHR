---
title: "Clustering Employees to Lower Turnover"
author: "Ricky Soo"
date: "January 7th, 2021"
output:
  html_document:
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: no
  pdf_document:
    toc: yes
    toc_depth: 2
---

This is a mini data science project in collaboration with Lim Lee Wen based on the Human Resources Data Set by Dr. Rich at <a href="https://www.kaggle.com/rhuebner/human-resources-data-set" target="_blank"> https://www.kaggle.com/rhuebner/human-resources-data-set</a>

## Introduction

To reduce loss of talent, the HR department can seek to understand the active employees by segmenting them into different groups. The member of each group is similar with one another, yet are dissimilar with members from other groups. By doing so, the company can gain insights into how the employees are similar and different from one another, and devise strategies in dealing with them.

```{r}
library(gower)
library(factoextra)
library(cluster)
library(clustertend)
library(dplyr)
```

```{r Load and prepare data}
# Make model repeatable
set.seed(315)

# Load data and select only active employees
df_active <- readRDS('HRDataset_v14-cleaned.rds') %>%
  filter(
    Termd == 0
  )

row.names(df_active) <- df_active$EmpID

str(df_active)
head(df_active)

```

## Data Preparation

Select the variables and scale the numeric variables for clustering. The salary variable is log-transformed as it is highly skewed.

```{r Scale data}

df_scaled <- as.data.frame(df_active)
df_scaled$Salary <- log10(df_scaled$Salary)

df_scaled <- df_scaled %>%
  dplyr::select(
    Salary,
    Position,
    State,
    Sex,
    MaritalDesc,
    CitizenDesc,
    HispanicLatino,
    RaceDesc,
    Department,
    ManagerName,
    RecruitmentSource,
    PerformanceScore,
    EngagementSurvey,
    EmpSatisfaction,
    SpecialProjectsCount,
    DaysLateLast30,
    Absences,
    TempAge,
    TempYrOfService
  )

num_cols <- colnames(df_scaled)[unlist(sapply(df_scaled, is.numeric))] 
df_scaled[num_cols] <- sapply(df_scaled[num_cols], scale)

str(df_scaled)
head(df_scaled)

# Confirm there is no missing value
sum(is.na(df_scaled))

```

## Gower Distance

The dissimilarity matrix is first generated using the Gower distance. Gower is used as we are dealing with mixed data types rather than just numeric variables, where Euclidean or Manhattan distance would be useful.

```{r Generate dissimilarity matrix}

dist <- matrix(0, ncol = nrow(df_scaled))
dist <- as.data.frame(dist)

for (i in 1:nrow(df_scaled)) {
  dist[i, ] <- gower_dist(df_scaled[i, ], df_scaled)
}

saveRDS(dist, 'dist.rds')
dist_mat <- as.matrix(dist)

```

## Clustering Tendency

The Hopkins statistic (Lawson and Jurs, 1990) and data visualization are used to assess whether the distance matrix is feasible for cluster analysis. If the Hopkins statistic is close to 0.5, or the data visualization shows random distribution of blue color, then there is no meaningful clusters.

The Hopkins statistic measures the probability that the data is generated by a uniform data distribution. In this case, the data is shown to be cohesive enough for clustering.

```{r Assess clustering tendency}

hopkins(dist_mat, n = nrow(dist_mat) - 1)
fviz_dist(as.dist(dist), show_labels = FALSE)

```

## Optimum Number of Clusters

3 methods are used to find the best number of clusters - Silhouette, Elbow method (within-cluster sum of square of WSS), and the Gap Statistic.

In this case, the Silhouette method suggests 2 clusters. The Elbow method suggest 3 clusters. The Gap Statistic suggests 4 clusters. A middle ground of 3 clusters is chosen as it fares well enough in all methods.

```{r Find optimum number of clusters}

fviz_nbclust(dist_mat, pam, method = 'silhouette')
fviz_nbclust(dist_mat, pam, method = 'wss')
fviz_nbclust(dist_mat, pam, method = 'gap_stat')

```

## Clustering Using PAM

Then the data is clustered into 3 clusters using Partitioning Around Medoids (PAM). It is a k-medoids algorithm in which data points are clustered around middle data point of each cluster, rather than mean value as in k-means algorithm.

The PAM algorithm is more robust to noise and less sensitive to outliers than k-means algorithm.


```{r Cluster using Partitioning Around Medoids (PAM)}

pam.res <- pam(dist_mat, k = 3)

fviz_cluster(
  pam.res,
  data = df_scaled,
  palette = 'jco',
  ellipse.type = 't',
  geom = 'point',
  start.plot = TRUE,
  repel = TRUE,
  ggtheme = theme_minimal()
)

saveRDS(pam.res, 'pam.rds')

```

## Evaluation of Clusters

The generated clusters are then evaluated using the Silhouette width method. It shows that the data points are well clustered with just a few data points showing negative Silhoutte widths.

```{r Evaluate clustering using Silhouette}
sil <- silhouette(pam.res$cluster, dist)
plot(sil)

fviz_silhouette(pam.res, palette = 'jco', ggtheme = theme_classic())
sil[sil[, 'sil_width'] < 0, ]

```
## Adding Cluster Info

The cluster number of each employee is then added back to the dataset. The means of each numeric variable is calculated to give an initial assessment of the resulting clusters.

```{r Adf_clustered cluster number to data}

df_clustered <- cbind(Cluster = pam.res$cluster, df_active)
df_clustered$Cluster <- as.factor(df_clustered$Cluster)
head(df_clustered)
saveRDS(df_clustered, 'HRDataset_v14-clustered.rds')

```
```{r Assess cluster characteristics}

# Show info of the medoids of each cluster
pam.res$clusinfo
df_clustered[pam.res$id.med, ]

# Show the means of numeric variables of each cluster
aggregate(df_active[, num_cols], by = list(CLUSTER = pam.res$cluster), mean)

```

## Visualizing Clusters

The clusters of employees are now visualized using bar plots and histograms. Each shows what makes them different from one another.

```{r Visualize cluster characteristics using bar plots and histograms}

theme_set(theme_bw())

ggplot(data = df_clustered, aes(x = Salary, fill = Cluster)) +
  geom_histogram()

```
```{r  as it is found to be highly skewed}
ggplot(data = df_clustered, aes(x = Salary, fill = Cluster)) +
  geom_histogram() +
  scale_x_log10()

ggplot(data = df_clustered, aes(y = Position, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(y = State, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(y = Zip, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = Sex, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = MaritalDesc, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = CitizenDesc, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = HispanicLatino, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = RaceDesc, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(y = Department, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(y = ManagerName, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(y = RecruitmentSource, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = PerformanceScore, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = EngagementSurvey, fill = Cluster)) +
  geom_histogram(binwidth = 0.5)

ggplot(data = df_clustered, aes(x = EmpSatisfaction, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = SpecialProjectsCount, fill = Cluster)) +
  geom_bar()

ggplot(data = df_clustered, aes(x = LastPerformanceReview_Date, fill = Cluster)) +
  geom_histogram()

ggplot(data = df_clustered, aes(x = DaysLateLast30, fill = Cluster)) +
  geom_histogram(binwidth = 1)

ggplot(data = df_clustered, aes(x = Absences, fill = Cluster)) +
  geom_histogram(binwidth = 1)

ggplot(data = df_clustered, aes(x = TempAge, fill = Cluster)) +
  geom_histogram(binwidth = 2)

ggplot(data = df_clustered, aes(x = TempYrOfService, fill = Cluster)) +
  geom_histogram(binwidth = 2)

```
```{r Visualize cluster characteristics using box plots}

ggplot(data = df_clustered, aes(x = Cluster, y = Salary, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = EngagementSurvey, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = SpecialProjectsCount, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = DaysLateLast30, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = TempAge, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = TempYrOfService, fill = Cluster)) +
  geom_boxplot()

ggplot(data = df_clustered, aes(x = Cluster, y = Absences, fill = Cluster)) +
  geom_boxplot()

```

### Interpreting Clusters

From the data visualization above, the variables below are found to differentiate the clusters from one another.

1. Salary - Cluster 2 has the highest salary, followed by cluster 3 and 1.
2. Position - Cluster 1 tends to have production-related positions, cluster 3 sales-related, and cluster 2 for the others.
3. Department - Cluster 1 tends to come from production department, cluster 3 from sales and executive office, and cluster 2 for the others.
4. Special projects - Cluster 2 has the highest number of special projects, followed by cluster 3 and 1.

In summary, the clustering model has segmented the employees into 3 clusters. Cluster 1 tend to be the blue-collar workers, Cluster 2 tend to be on sales and marketing, and Cluster 3 the white-collar workers.
